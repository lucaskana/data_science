{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /io/opencv/modules/core/src/matrix.cpp:810: error: (-215:Assertion failed) 0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows in function 'Mat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75e959020b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# resize image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseamlessClone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORMAL_CLONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"background position {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /io/opencv/modules/core/src/matrix.cpp:810: error: (-215:Assertion failed) 0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows in function 'Mat'\n"
     ]
    }
   ],
   "source": [
    "#https://google.github.io/mediapipe/\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "import numpy as np\n",
    "\n",
    "src = cv2.imread(\"Cropped Image.jpg\")\n",
    "# Define an array of endpoints of triangle\n",
    "points = np.array([[14, 14],[160, 1],[158, 17],[126, 45],[95, 52],[54, 52],[22 ,28]], np.int32)\n",
    "\n",
    "foreground = np.ones((100,100,3),dtype='uint8')*255\n",
    "# Set initial value of weights\n",
    "alpha = 0.4\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print and draw face mesh landmarks on the image.\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "      print('face_landmarks:', face_landmarks)\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_tesselation_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_contours_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "    \n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "# https://www.youtube.com/watch?v=Yg6bFRnOSbs\n",
    "\n",
    "    # Draw the face mesh annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "        \n",
    "        src_mask = np.zeros(src.shape, src.dtype)\n",
    "\n",
    "        #image = cv2.imread('sorriso.jpeg')\n",
    "        rgb_image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        source = face_landmarks.landmark[13]\n",
    "        target = face_landmarks.landmark[14]\n",
    "        relative_source = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "        relative_target = (int(target.x * image.shape[1]), int(target.y * image.shape[0]))\n",
    "        center = (int((relative_source[0] + relative_target[0])/2),int((relative_source[1] + relative_target[1])/2)-5)\n",
    "    \n",
    "    cv2.fillPoly(src_mask, [points], (255, 255, 255))\n",
    "\n",
    "    scale_percent = 60 # percent of original size\n",
    "    width = int(src.shape[1] * scale_percent / 100)\n",
    "    height = int(src.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "      \n",
    "    # resize image\n",
    "    resized = cv2.resize(src, dim, interpolation = cv2.INTER_AREA)\n",
    "    image = cv2.seamlessClone(src, image, src_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "    print(\"background position {}\".format(center))\n",
    "    #background = image\n",
    "    #background = cv2.flip(image, 1)\n",
    "    #added_image = cv2.addWeighted(background[center[1]-50:center[1]+50,center[0]-50:center[0]+50,:],alpha,foreground[0:100,0:100,:],1-alpha,0)\n",
    "    #background[center[1]-50:center[1]+50,center[0]-50:center[0]+50] = added_image\n",
    "    #cv2.imshow('MediaPipe Face Mesh', cv2.flip(background, 1))\n",
    "    #background = cv2.flip(background, 1)\n",
    "    cv2.imshow('MediaPipe Face Mesh', image)\n",
    "    #cv2.imshow('MediaPipe Face Mesh', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start tutorial\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "#mp_drawing_styles = mp.solutions.drawing_styles\n",
    "#mp_face_mesh = mp.solutions.face_mesh\n",
    "# https://www.youtube.com/watch?v=mCcPmlr7y3U&t=2s\n",
    "\n",
    "webcam = cv2.VideoCapture(0)\n",
    "sol_rec_face = mp.solutions.face_detection\n",
    "rec_face = sol_rec_face.FaceDetection()\n",
    "desenho = mp.solutions.drawing_utils\n",
    "\n",
    "print(\"start tutorial\")\n",
    "\n",
    "while True:\n",
    "    verify, frame =  webcam.read()\n",
    "\n",
    "    if not verify:\n",
    "        break\n",
    "\n",
    "    face_list = rec_face.process(frame)\n",
    "\n",
    "    if face_list.detections:\n",
    "        for face in face_list.detections:\n",
    "            desenho.draw_detection(frame, face)\n",
    "\n",
    "    cv2.imshow(\"faces\", frame)\n",
    "\n",
    "    # quando aperta ESC finaliza\n",
    "    if cv2.waitKey(5) == 27:\n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({(270, 409), (317, 402), (81, 82), (91, 181), (37, 0), (84, 17), (269, 270), (321, 375), (318, 324), (312, 311), (415, 308), (17, 314), (61, 146), (78, 95), (0, 267), (82, 13), (314, 405), (178, 87), (267, 269), (61, 185), (14, 317), (88, 178), (185, 40), (405, 321), (13, 312), (324, 308), (409, 291), (146, 91), (87, 14), (78, 191), (95, 88), (311, 310), (39, 37), (40, 39), (402, 318), (191, 80), (80, 81), (310, 415), (181, 84), (375, 291)})\n",
      "x: 0.5226072669029236\n",
      "y: 0.6036577224731445\n",
      "z: -0.018408799543976784\n",
      "\n",
      "(522, 402)\n",
      "(543, 399)\n"
     ]
    }
   ],
   "source": [
    "from unittest import skip\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "#mp_drawing = mp.solutions.drawing_utils\n",
    "#mp_drawing_styles = mp.solutions.drawing_styles\n",
    "#mp_face_mesh = mp.solutions.face_mesh\n",
    "# https://www.youtube.com/watch?v=mCcPmlr7y3U&t=2s\n",
    "\n",
    "#mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "image = cv2.imread('sorriso.jpeg')\n",
    "rgb_image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "height, width, _ = image.shape\n",
    "\n",
    "results = face_mesh.process(rgb_image)\n",
    "\n",
    "#type(results.multi_face_landmarks)\n",
    "#results.multi_face_landmarks\n",
    "\n",
    "#mp_face_mesh.FACEMESH_LIPS\n",
    "\n",
    "facial_landmarks = results.multi_face_landmarks[0]\n",
    "#for facial_landmarks in results.multi_face_landmarks:\n",
    "#    pt1 = facial_landmarks.landmark[0]\n",
    "#    x = int(pt1.x * width)\n",
    "#    y = int(pt1.y * height)\n",
    "#    cv2.circle(image, (x,y), 7 ,(100,100,0), -1)\n",
    "\n",
    "\n",
    "#print(type(mp_face_mesh.FACEMESH_LIPS))\n",
    "print(mp_face_mesh.FACEMESH_LIPS)\n",
    "count = 0\n",
    "for source_idx, target_idx in mp_face_mesh.FACEMESH_LIPS:\n",
    "    \n",
    "    #if count > 20:\n",
    "    #    print('enter')\n",
    "    #else:\n",
    "    source = facial_landmarks.landmark[source_idx]\n",
    "    target = facial_landmarks.landmark[target_idx]\n",
    "    relative_source = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "    relative_target = (int(target.x * image.shape[1]), int(target.y * image.shape[0]))\n",
    "    #if source_idx < 150 and target_idx < 100:\n",
    "    if source_idx == 13:\n",
    "        \n",
    "        print(source)\n",
    "\n",
    "        \n",
    "        #cv2.line(image, relative_source,relative_target, color= (255,255,255), thickness= 2)\n",
    "        print(relative_source)\n",
    "        print(relative_target)\n",
    "        cv2.circle(image, relative_source, radius=4, color=(0, 0, 255), thickness=-1)\n",
    "        #cv2.circle(image, relative_target, radius=4, color=(0, 255, 0), thickness=-1)\n",
    "        \n",
    "        # font\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        \n",
    "        # org\n",
    "        org = (50, 50)\n",
    "        \n",
    "        # fontScale\n",
    "        fontScale = .5\n",
    "        \n",
    "        # Blue color in BGR\n",
    "        color = (255, 0, 0)\n",
    "        \n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "        \n",
    "        # Using cv2.putText() method\n",
    "        image = cv2.putText(image, str(source_idx), relative_source, font, \n",
    "                        fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    #cv2.line(image, relative_source,relative_target, color= (255,255,255), thickness= 2)\n",
    "\n",
    "cv2.imshow(\"Image\",image)\n",
    "#cv2.imshow(\"Image\",rgb_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'facial_landmarks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-941cf64ba124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfacial_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfacial_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mrelative_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'facial_landmarks' is not defined"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "# https://learnopencv.com/seamless-cloning-using-opencv-python-cpp/\n",
    "# Read images\n",
    "src = cv2.imread(\"Cropped Image.jpg\")\n",
    "# Define an array of endpoints of triangle\n",
    "points = np.array([[14, 14],[160, 1],[158, 17],[126, 45],[95, 52],[54, 52],[22 ,28]], np.int32)\n",
    "#dst = cv2.imread(\"clouds.jpg\")\n",
    "dst = cv2.imread(\"sorriso_teste.jpg\")\n",
    "\n",
    "\n",
    "#image = cv2.circle(image, (x,y), radius=0, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "# Define an array of endpoints of triangle\n",
    "points = np.array([[14, 14],[160, 1],[158, 17],[126, 45],[95, 52],[54, 52],[22 ,28]], np.int32)\n",
    "\n",
    "#print(src.shape)\n",
    "#print(dst.shape)\n",
    "\n",
    "# Create a rough mask around the airplane.\n",
    "src_mask = np.zeros(src.shape, src.dtype)\n",
    "#poly = np.array([ [4,80], [30,54], [151,63], [254,37], [298,90], [272,134], [43,122] ], np.int32)\n",
    "cv2.fillPoly(src_mask, [points], (255, 255, 255))\n",
    "\n",
    "# This is where the CENTER of the airplane will be placed\n",
    "center = (100,100)\n",
    "\n",
    "source = facial_landmarks.landmark[13]\n",
    "target = facial_landmarks.landmark[14]\n",
    "relative_source = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "relative_target = (int(target.x * image.shape[1]), int(target.y * image.shape[0]))\n",
    "print(relative_source[0])\n",
    "print(relative_target)\n",
    "center = (int((relative_source[0] + relative_target[0])/2),int((relative_source[1] + relative_target[1])/2)-5)\n",
    "print(center)\n",
    "# Clone seamlessly.\n",
    "output = cv2.seamlessClone(src, dst, src_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "# Save result\n",
    "cv2.imwrite(\"fix_smile_example.jpg\", output);\n",
    "\n",
    "result = cv2.imread(\"fix_smile_example.jpg\")\n",
    "cv2.imshow(\"Sorriso Teste\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 1000, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-91133371df7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cropped Image.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('sorriso.jpeg')\n",
    "print(img.shape) # Print image shape\n",
    "cv2.imshow(\"original\", img)\n",
    "\n",
    "# Cropping an image\n",
    "cropped_image = img[384:450, 435:596]\n",
    "\n",
    "# Display cropped image\n",
    "cv2.imshow(\"cropped\", cropped_image)\n",
    "\n",
    "# Save the cropped image\n",
    "cv2.imwrite(\"Cropped Image.jpg\", cropped_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# create an overlay image. You can use any image\n",
    "foreground = np.ones((100,100,3),dtype='uint8')*255\n",
    "# Open the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set initial value of weights\n",
    "alpha = 0.4\n",
    "while True:\n",
    "    # read the background\n",
    "    ret, background = cap.read()\n",
    "    background = cv2.flip(background,1)\n",
    "    # Select the region in the background where we want to add the image and add the images using cv2.addWeighted()\n",
    "    added_image = cv2.addWeighted(background[150:250,150:250,:],alpha,foreground[0:100,0:100,:],1-alpha,0)\n",
    "    # Change the region with the result\n",
    "    background[150:250,150:250] = added_image\n",
    "    # For displaying current value of alpha(weights)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(background,'alpha:{}'.format(alpha),(10,30), font, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "    cv2.imshow('a',background)\n",
    "    k = cv2.waitKey(10)\n",
    "    # Press q to break\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "    # press a to increase alpha by 0.1\n",
    "    if k == ord('a'):\n",
    "        alpha +=0.1\n",
    "        if alpha >=1.0:\n",
    "            alpha = 1.0\n",
    "    # press d to decrease alpha by 0.1\n",
    "    elif k== ord('d'):\n",
    "        alpha -= 0.1\n",
    "        if alpha <=0.0:\n",
    "            alpha = 0.0\n",
    "# Release the camera and destroy all windows         \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = cv2.imread('sorriso.jpeg')\n",
    "\n",
    "#background = cv2.flip(background,1)\n",
    "foreground = np.ones((100,100,3),dtype='uint8')*255\n",
    "added_image = cv2.addWeighted(background[150:250,150:250,:],alpha,foreground[0:100,0:100,:],1-alpha,0)\n",
    "background[150:250,150:250] = added_image\n",
    "\n",
    "cv2.imshow(\"original\", background)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "(338, 389)\n",
      "(338, 383)\n",
      "337\n",
      "(337, 389)\n",
      "(337, 383)\n",
      "337\n",
      "(337, 388)\n",
      "(337, 383)\n",
      "338\n",
      "(338, 388)\n",
      "(338, 383)\n",
      "338\n",
      "(338, 387)\n",
      "(338, 382)\n",
      "337\n",
      "(337, 386)\n",
      "(337, 381)\n",
      "339\n",
      "(339, 386)\n",
      "(339, 381)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "340\n",
      "(339, 383)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(339, 381)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 376)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "340\n",
      "(340, 382)\n",
      "(340, 376)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(338, 383)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(339, 382)\n",
      "(339, 377)\n",
      "338\n",
      "(338, 382)\n",
      "(338, 377)\n",
      "339\n",
      "(339, 383)\n",
      "(339, 378)\n",
      "339\n",
      "(339, 383)\n",
      "(339, 378)\n",
      "338\n",
      "(338, 383)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 383)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 383)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "339\n",
      "(339, 384)\n",
      "(339, 379)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "338\n",
      "(338, 384)\n",
      "(338, 379)\n",
      "338\n",
      "(338, 383)\n",
      "(338, 378)\n",
      "338\n",
      "(338, 381)\n",
      "(338, 376)\n",
      "337\n",
      "(338, 379)\n",
      "(337, 374)\n",
      "336\n",
      "(336, 376)\n",
      "(336, 371)\n",
      "334\n",
      "(335, 375)\n",
      "(334, 370)\n",
      "333\n",
      "(333, 372)\n",
      "(333, 367)\n",
      "330\n",
      "(330, 370)\n",
      "(330, 364)\n",
      "327\n",
      "(327, 367)\n",
      "(327, 362)\n",
      "324\n",
      "(324, 365)\n",
      "(324, 360)\n",
      "321\n",
      "(321, 363)\n",
      "(321, 358)\n",
      "320\n",
      "(320, 362)\n",
      "(320, 357)\n",
      "319\n",
      "(319, 362)\n",
      "(319, 357)\n",
      "319\n",
      "(319, 363)\n",
      "(319, 358)\n",
      "320\n",
      "(320, 366)\n",
      "(320, 358)\n",
      "321\n",
      "(321, 369)\n",
      "(321, 358)\n",
      "322\n",
      "(322, 369)\n",
      "(322, 356)\n",
      "321\n",
      "(321, 367)\n",
      "(321, 353)\n",
      "320\n",
      "(321, 368)\n",
      "(320, 352)\n",
      "322\n",
      "(323, 368)\n",
      "(322, 352)\n",
      "324\n",
      "(324, 369)\n",
      "(324, 353)\n",
      "322\n",
      "(322, 368)\n",
      "(322, 351)\n",
      "323\n",
      "(323, 368)\n",
      "(323, 351)\n",
      "324\n",
      "(324, 366)\n",
      "(324, 352)\n",
      "322\n",
      "(322, 364)\n",
      "(322, 354)\n",
      "321\n",
      "(321, 359)\n",
      "(321, 354)\n",
      "320\n",
      "(321, 359)\n",
      "(320, 354)\n",
      "321\n",
      "(321, 359)\n",
      "(321, 354)\n",
      "322\n",
      "(322, 358)\n",
      "(322, 353)\n",
      "323\n",
      "(323, 358)\n",
      "(323, 353)\n",
      "323\n",
      "(323, 365)\n",
      "(323, 356)\n",
      "325\n",
      "(324, 367)\n",
      "(324, 356)\n",
      "325\n",
      "(325, 368)\n",
      "(325, 355)\n",
      "325\n",
      "(325, 368)\n",
      "(325, 354)\n",
      "329\n",
      "(329, 370)\n",
      "(329, 354)\n",
      "326\n",
      "(326, 370)\n",
      "(326, 353)\n",
      "326\n",
      "(326, 371)\n",
      "(326, 354)\n",
      "327\n",
      "(327, 373)\n",
      "(327, 355)\n",
      "330\n",
      "(329, 372)\n",
      "(329, 356)\n",
      "332\n",
      "(332, 373)\n",
      "(332, 358)\n",
      "334\n",
      "(334, 375)\n",
      "(334, 359)\n",
      "336\n",
      "(336, 376)\n",
      "(336, 360)\n",
      "338\n",
      "(337, 377)\n",
      "(337, 361)\n",
      "339\n",
      "(338, 377)\n",
      "(338, 361)\n",
      "338\n",
      "(337, 376)\n",
      "(337, 361)\n",
      "338\n",
      "(338, 378)\n",
      "(338, 362)\n",
      "337\n",
      "(337, 377)\n",
      "(337, 361)\n",
      "333\n",
      "(333, 375)\n",
      "(333, 359)\n",
      "327\n",
      "(327, 374)\n",
      "(327, 359)\n",
      "318\n",
      "(318, 375)\n",
      "(318, 360)\n",
      "307\n",
      "(308, 375)\n",
      "(307, 360)\n",
      "297\n",
      "(298, 375)\n",
      "(297, 360)\n",
      "287\n",
      "(289, 376)\n",
      "(288, 361)\n",
      "281\n",
      "(283, 377)\n",
      "(282, 362)\n",
      "277\n",
      "(278, 377)\n",
      "(277, 363)\n",
      "270\n",
      "(274, 381)\n",
      "(272, 365)\n",
      "269\n",
      "(272, 382)\n",
      "(270, 366)\n",
      "265\n",
      "(269, 383)\n",
      "(267, 367)\n",
      "263\n",
      "(267, 383)\n",
      "(265, 367)\n",
      "263\n",
      "(268, 382)\n",
      "(265, 367)\n",
      "266\n",
      "(270, 382)\n",
      "(268, 366)\n",
      "269\n",
      "(273, 380)\n",
      "(271, 365)\n",
      "276\n",
      "(278, 376)\n",
      "(277, 362)\n",
      "286\n",
      "(289, 374)\n",
      "(287, 360)\n",
      "300\n",
      "(301, 371)\n",
      "(300, 358)\n",
      "314\n",
      "(314, 370)\n",
      "(314, 357)\n",
      "330\n",
      "(330, 372)\n",
      "(330, 358)\n",
      "346\n",
      "(344, 373)\n",
      "(345, 359)\n",
      "362\n",
      "(359, 378)\n",
      "(360, 364)\n",
      "378\n",
      "(373, 380)\n",
      "(375, 367)\n",
      "387\n",
      "(383, 383)\n",
      "(385, 373)\n",
      "394\n",
      "(393, 383)\n",
      "(393, 376)\n",
      "398\n",
      "(398, 382)\n",
      "(398, 377)\n",
      "399\n",
      "(399, 381)\n",
      "(399, 376)\n",
      "402\n",
      "(402, 381)\n",
      "(402, 376)\n",
      "404\n",
      "(404, 382)\n",
      "(404, 377)\n",
      "403\n",
      "(403, 381)\n",
      "(403, 376)\n",
      "399\n",
      "(399, 379)\n",
      "(399, 374)\n",
      "397\n",
      "(397, 379)\n",
      "(397, 373)\n",
      "395\n",
      "(395, 378)\n",
      "(395, 373)\n",
      "392\n",
      "(392, 377)\n",
      "(392, 372)\n",
      "392\n",
      "(392, 377)\n",
      "(392, 372)\n",
      "393\n",
      "(393, 377)\n",
      "(393, 372)\n",
      "391\n",
      "(391, 377)\n",
      "(391, 372)\n",
      "391\n",
      "(392, 377)\n",
      "(391, 372)\n",
      "390\n",
      "(390, 377)\n",
      "(390, 372)\n",
      "393\n",
      "(393, 377)\n",
      "(393, 372)\n",
      "388\n",
      "(388, 375)\n",
      "(388, 370)\n",
      "386\n",
      "(386, 372)\n",
      "(386, 367)\n",
      "383\n",
      "(383, 370)\n",
      "(383, 365)\n",
      "380\n",
      "(379, 368)\n",
      "(379, 363)\n",
      "376\n",
      "(376, 367)\n",
      "(376, 362)\n",
      "373\n",
      "(373, 367)\n",
      "(373, 362)\n",
      "369\n",
      "(369, 367)\n",
      "(369, 362)\n",
      "366\n",
      "(366, 367)\n",
      "(366, 362)\n",
      "362\n",
      "(362, 367)\n",
      "(362, 362)\n",
      "360\n",
      "(360, 368)\n",
      "(360, 363)\n",
      "358\n",
      "(358, 369)\n",
      "(358, 364)\n",
      "357\n",
      "(357, 371)\n",
      "(357, 365)\n",
      "356\n",
      "(356, 372)\n",
      "(356, 367)\n",
      "356\n",
      "(356, 372)\n",
      "(356, 367)\n",
      "355\n",
      "(355, 373)\n",
      "(355, 368)\n",
      "356\n",
      "(356, 374)\n",
      "(356, 369)\n",
      "355\n",
      "(355, 374)\n",
      "(355, 369)\n",
      "356\n",
      "(356, 375)\n",
      "(356, 370)\n",
      "356\n",
      "(356, 376)\n",
      "(356, 371)\n",
      "356\n",
      "(357, 376)\n",
      "(356, 370)\n",
      "356\n",
      "(356, 376)\n",
      "(356, 371)\n",
      "356\n",
      "(356, 377)\n",
      "(356, 371)\n",
      "356\n",
      "(356, 376)\n",
      "(356, 371)\n",
      "356\n",
      "(356, 377)\n",
      "(356, 372)\n",
      "356\n",
      "(356, 377)\n",
      "(356, 372)\n",
      "355\n",
      "(355, 377)\n",
      "(355, 372)\n",
      "355\n",
      "(355, 377)\n",
      "(355, 372)\n",
      "355\n",
      "(355, 377)\n",
      "(355, 372)\n",
      "355\n",
      "(355, 377)\n",
      "(355, 372)\n",
      "354\n",
      "(354, 377)\n",
      "(354, 372)\n",
      "355\n",
      "(355, 377)\n",
      "(355, 372)\n",
      "354\n",
      "(354, 377)\n",
      "(354, 371)\n",
      "354\n",
      "(354, 376)\n",
      "(354, 371)\n",
      "354\n",
      "(354, 376)\n",
      "(354, 371)\n",
      "353\n",
      "(354, 376)\n",
      "(353, 370)\n",
      "354\n",
      "(354, 375)\n",
      "(354, 370)\n",
      "352\n",
      "(352, 374)\n",
      "(352, 369)\n",
      "351\n",
      "(351, 374)\n",
      "(351, 369)\n",
      "350\n",
      "(350, 373)\n",
      "(350, 368)\n",
      "350\n",
      "(350, 373)\n",
      "(350, 368)\n",
      "350\n",
      "(350, 373)\n",
      "(350, 368)\n",
      "349\n",
      "(349, 373)\n",
      "(349, 368)\n",
      "349\n",
      "(349, 373)\n",
      "(349, 368)\n",
      "349\n",
      "(349, 374)\n",
      "(349, 368)\n",
      "350\n",
      "(350, 374)\n",
      "(350, 369)\n",
      "349\n",
      "(349, 375)\n",
      "(349, 370)\n",
      "350\n",
      "(350, 374)\n",
      "(350, 369)\n",
      "349\n",
      "(349, 375)\n",
      "(349, 370)\n",
      "348\n",
      "(348, 378)\n",
      "(348, 373)\n",
      "348\n",
      "(348, 378)\n",
      "(348, 372)\n",
      "347\n",
      "(347, 376)\n",
      "(347, 371)\n",
      "347\n",
      "(347, 374)\n",
      "(347, 369)\n",
      "347\n",
      "(347, 375)\n",
      "(347, 370)\n",
      "346\n",
      "(346, 374)\n",
      "(346, 369)\n",
      "345\n",
      "(345, 373)\n",
      "(345, 368)\n",
      "344\n",
      "(344, 374)\n",
      "(344, 369)\n",
      "343\n",
      "(343, 374)\n",
      "(343, 369)\n",
      "343\n",
      "(344, 373)\n",
      "(343, 368)\n",
      "344\n",
      "(344, 373)\n",
      "(344, 368)\n",
      "343\n",
      "(343, 373)\n",
      "(343, 368)\n",
      "343\n",
      "(344, 371)\n",
      "(343, 366)\n",
      "344\n",
      "(344, 371)\n",
      "(344, 366)\n",
      "344\n",
      "(344, 372)\n",
      "(344, 366)\n",
      "344\n",
      "(344, 371)\n",
      "(344, 366)\n",
      "345\n",
      "(345, 372)\n",
      "(345, 367)\n",
      "347\n",
      "(347, 373)\n",
      "(347, 368)\n",
      "348\n",
      "(348, 375)\n",
      "(348, 370)\n",
      "348\n",
      "(348, 375)\n",
      "(348, 370)\n"
     ]
    }
   ],
   "source": [
    "#https://google.github.io/mediapipe/\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "foreground = np.ones((100,100,3),dtype='uint8')*255\n",
    "# Set initial value of weights\n",
    "alpha = 0.4\n",
    "    \n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "# https://www.youtube.com/watch?v=Yg6bFRnOSbs\n",
    "# https://www.youtube.com/watch?v=ObmuflX8Ank\n",
    "\n",
    "    # Draw the face mesh annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "        src = cv2.imread(\"Cropped Image.jpg\")\n",
    "        src_mask = np.zeros(src.shape, src.dtype)\n",
    "\n",
    "        #image = cv2.imread('sorriso.jpeg')\n",
    "        rgb_image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        source = face_landmarks.landmark[13]\n",
    "        target = face_landmarks.landmark[14]\n",
    "        relative_source = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "        relative_target = (int(target.x * image.shape[1]), int(target.y * image.shape[0]))\n",
    "        center = (int((relative_source[0] + relative_target[0])/2),int((relative_source[1] + relative_target[1])/2)-5)\n",
    "        print(center)\n",
    "\n",
    "        \n",
    "\n",
    "        # Clone seamlessly.\n",
    "        output = cv2.seamlessClone(src, image, src_mask, center, cv2.NORMAL_CLONE)\n",
    "        #if face_landmarks in mp_face_mesh.FACEMESH_LIPS:\n",
    "        #mp_drawing.draw_landmarks(\n",
    "        #    image=image,\n",
    "        #    landmark_list=face_landmarks,\n",
    "        #    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        #    landmark_drawing_spec=None,\n",
    "        #    connection_drawing_spec=mp_drawing_styles\n",
    "        #    .get_default_face_mesh_tesselation_style())\n",
    "        #mp_drawing.draw_landmarks(\n",
    "        #    image=image,\n",
    "        #    landmark_list=face_landmarks,\n",
    "        #    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        #    landmark_drawing_spec=None,\n",
    "        #    connection_drawing_spec=mp_drawing_styles\n",
    "        #    .get_default_face_mesh_contours_style())\n",
    "        #mp_drawing.draw_landmarks(\n",
    "        #    image=image,\n",
    "        #    landmark_list=face_landmarks,\n",
    "        #    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        #    landmark_drawing_spec=None,\n",
    "        #    connection_drawing_spec=mp_drawing_styles\n",
    "        #    .get_default_face_mesh_iris_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "\n",
    "    #background = cv2.flip(image, 1)\n",
    "    # Select the region in the background where we want to add the image and add the images using cv2.addWeighted()\n",
    "    #added_image = cv2.addWeighted(background[150:250,150:250,:],alpha,foreground[0:100,0:100,:],1-alpha,0)\n",
    "    # Change the region with the result\n",
    "    #background[150:250,150:250] = added_image\n",
    "\n",
    "    cv2.imshow('MediaPipe Face Mesh', cv2.flip(output, 1))\n",
    "    #cv2.imshow('MediaPipe Face Mesh', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.9.0.1-cp39-cp39-win_amd64.whl (49.8 MB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (3.4.3)\n",
      "Collecting protobuf<4,>=3.11\n",
      "  Using cached protobuf-3.20.3-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from mediapipe) (1.20.3)\n",
      "Collecting opencv-contrib-python\n",
      "  Using cached opencv_contrib_python-4.7.0.68-cp37-abi3-win_amd64.whl (44.9 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->mediapipe) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->mediapipe) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\lucas\\appdata\\roaming\\python\\python39\\site-packages (from cycler>=0.10->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: flatbuffers, protobuf, opencv-contrib-python, absl-py, mediapipe\n",
      "Successfully installed absl-py-1.3.0 flatbuffers-23.1.4 mediapipe-0.9.0.1 opencv-contrib-python-4.7.0.68 protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def facedetection():\n",
    "    file = \"sorriso.jpeg\"\n",
    "\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        image = cv2.imread(file)\n",
    "\n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        # Draw face detections of each face.\n",
    "        if results.detections:     \n",
    "            annotated_image = image.copy()\n",
    "            for detection in results.detections:\n",
    "                print('Nose tip:')\n",
    "                print(mp_face_detection.get_key_point(\n",
    "                    detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "                mp_drawing.draw_detection(annotated_image, detection)\n",
    "        cv2.imwrite('annotated_image.png', annotated_image)\n",
    "\n",
    "\n",
    "\n",
    "# For static images:\n",
    "def drawing(filename):\n",
    "  IMAGE_FILES = [filename]\n",
    "  drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "  with mp_face_mesh.FaceMesh(\n",
    "      static_image_mode=True,\n",
    "      max_num_faces=1,\n",
    "      refine_landmarks=True,\n",
    "      min_detection_confidence=0.5) as face_mesh:\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "      image = cv2.imread(file)\n",
    "      # Convert the BGR image to RGB before processing.\n",
    "      results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "      # Print and draw face mesh landmarks on the image.\n",
    "      if not results.multi_face_landmarks:\n",
    "        continue\n",
    "      annotated_image = image.copy()\n",
    "      #print(results.multi_face_landmarks)\n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "        #print('face_landmarks:', face_landmarks)\n",
    "        #print(mp_face_mesh.FACEMESH_LIPS)\n",
    "        #mp_drawing.draw_landmarks(\n",
    "        #    image=annotated_image,\n",
    "        #    landmark_list=face_landmarks,\n",
    "        #    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        #    landmark_drawing_spec=None,\n",
    "        #    connection_drawing_spec=mp_drawing_styles\n",
    "        #    .get_default_face_mesh_tesselation_style())\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_LIPS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        #mp_drawing.draw_landmarks(\n",
    "        #    image=annotated_image,\n",
    "        #    landmark_list=face_landmarks,\n",
    "        #    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        #    landmark_drawing_spec=None,\n",
    "        #    connection_drawing_spec=mp_drawing_styles\n",
    "        #    .get_default_face_mesh_iris_connections_style())\n",
    "      cv2.imwrite('annotated_image' + str(idx) + '.png', annotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh      \n",
    "\n",
    "# Read Images\n",
    "image = cv2.imread(\"../data/bush.jpeg\")\n",
    "body_im = cv2.imread(\"../data/sorriso.jpeg\")\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "      static_image_mode=True,\n",
    "      max_num_faces=1,\n",
    "      refine_landmarks=True,\n",
    "      min_detection_confidence=0.5) as face_mesh:\n",
    "  # Convert the BGR image to RGB before processing.\n",
    "  results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "  \n",
    "  mask_in = np.zeros(image.shape[0:2], dtype='uint8')\n",
    "  points = []\n",
    "  # Print and draw face mesh landmarks on the image.\n",
    "  if results.multi_face_landmarks:\n",
    "    \n",
    "    annotated_image = image.copy()\n",
    "    #print(results.multi_face_landmarks)\n",
    "\n",
    "    face_landmarks = results.multi_face_landmarks[0]\n",
    "    \n",
    "    for source_idx, target_idx in mp_face_mesh.FACEMESH_LIPS:\n",
    "\n",
    "        source = face_landmarks.landmark[source_idx]\n",
    "        target = face_landmarks.landmark[target_idx]\n",
    "\n",
    "        relative_source = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "        relative_target = (int(source.x * image.shape[1]), int(source.y * image.shape[0]))\n",
    "\n",
    "        points.append(relative_source)\n",
    "        points.append(relative_target)\n",
    "\n",
    "array_np = np.array([[e[0], e[1]] for e in points])  \n",
    "\n",
    "cv2.fillConvexPoly(mask_in,cv2.convexHull(array_np),color=(255,255,255))\n",
    "\n",
    "\n",
    "#Mask = cv2.bitwise_not(mask_in)\n",
    "#resultimage = cv2.bitwise_and(image, image, mask = Mask)\n",
    "\n",
    "#pts1 = np.float32([[50, 50],\n",
    "#                   [200, 50], \n",
    "#                   [50, 200]])\n",
    "  \n",
    "#pts2 = np.float32([[10, 100],\n",
    "#                   [200, 50], \n",
    "#                   [100, 250]])\n",
    "\n",
    "# Calculate the transformation matrix using cv2.getAffineTransform()\n",
    "#M= cv2.getAffineTransform(pts1 , pts2)\n",
    "\n",
    "#combined_image = image * mask_in + body_im * (1 - mask_in)\n",
    "\n",
    "\n",
    "#print(f\"The shape of image 1 and image 2 is {body_im.shape} and {resultimage.shape} respectively\")\n",
    "\n",
    "\n",
    "# The location of the center of the src in the dst\n",
    "width, height, channels = body_im.shape\n",
    "center = (int(height/2), int(width/2))\n",
    " \n",
    "# Seamlessly clone src into dst and put the results in output\n",
    "normal_clone = cv2.seamlessClone(image, body_im, mask_in, center, cv2.NORMAL_CLONE)\n",
    "#mixed_clone = cv2.seamlessClone(obj, im, mask, center, cv2.MIXED_CLONE)\n",
    "\n",
    "\n",
    "cv2.imshow(\"mask definition\", normal_clone)\n",
    "  \n",
    "# Displaying the image\n",
    "#cv2.imshow('Mask', Mask)\n",
    "  \n",
    "#rows, cols = body_im.shape[:2]\n",
    "#warped_face = cv2.warpAffine(image,M, (cols,rows))\n",
    "\n",
    "# waits for user to press any key\n",
    "# (this is necessary to avoid Python kernel form crashing)\n",
    "cv2.waitKey(0)\n",
    "# closing all open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# https://www.youtube.com/watch?v=ObmuflX8Ank\n",
    "# https://www.youtube.com/watch?v=Yg6bFRnOSbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(847, 640, 3)\n",
      "[[ 10. 100.]\n",
      " [200.  50.]\n",
      " [100. 250.]]\n",
      "[[  1.26666667   0.6        -83.33333333]\n",
      " [ -0.33333333   1.          66.66666667]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread('../data/bush.jpeg')\n",
    "rows, cols = img.shape[:2]\n",
    "\n",
    "# Define the 3 pairs of corresponding points \n",
    "pts1 = np.float32([[50, 50],\n",
    "                   [200, 50], \n",
    "                   [50, 200]])\n",
    "  \n",
    "pts2 = np.float32([[10, 100],\n",
    "                   [200, 50], \n",
    "                   [100, 250]])\n",
    "\n",
    "# Calculate the transformation matrix using cv2.getAffineTransform()\n",
    "M= cv2.getAffineTransform(pts1 , pts2)\n",
    "print(img.shape)\n",
    "print(pts2)\n",
    "print(M)\n",
    "\n",
    "# Apply the affine transformation using cv2.warpAffine()\n",
    "dst = cv2.warpAffine(img, M, (cols,rows))\n",
    "\n",
    "# Display the image\n",
    "out = cv2.hconcat([img, dst])\n",
    "cv2.imshow('Output', out)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source idx 317\n",
      "target_idx idx 402\n",
      "source idx 81\n",
      "target_idx idx 82\n",
      "source idx 13\n",
      "target_idx idx 312\n",
      "source idx 318\n",
      "target_idx idx 324\n",
      "source idx 324\n",
      "target_idx idx 308\n",
      "source idx 312\n",
      "target_idx idx 311\n",
      "source idx 415\n",
      "target_idx idx 308\n",
      "source idx 78\n",
      "target_idx idx 95\n",
      "source idx 87\n",
      "target_idx idx 14\n",
      "source idx 78\n",
      "target_idx idx 191\n",
      "source idx 82\n",
      "target_idx idx 13\n",
      "source idx 95\n",
      "target_idx idx 88\n",
      "source idx 311\n",
      "target_idx idx 310\n",
      "source idx 402\n",
      "target_idx idx 318\n",
      "source idx 191\n",
      "target_idx idx 80\n",
      "source idx 80\n",
      "target_idx idx 81\n",
      "source idx 178\n",
      "target_idx idx 87\n",
      "source idx 310\n",
      "target_idx idx 415\n",
      "source idx 14\n",
      "target_idx idx 317\n",
      "source idx 88\n",
      "target_idx idx 178\n",
      "(667, 1000, 3)\n",
      "496\n",
      "501\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh      \n",
    "\n",
    "# Leitura entrada\n",
    "image_in = cv2.imread(\"../data/bush.jpeg\")\n",
    "\n",
    "# Leitura Saida\n",
    "image_out = cv2.imread(\"../data/sorriso.jpeg\")\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "      static_image_mode=True,\n",
    "      max_num_faces=1,\n",
    "      refine_landmarks=True,\n",
    "      min_detection_confidence=0.5) as face_mesh:\n",
    "  # Convert the BGR image to RGB before processing.\n",
    "  results = face_mesh.process(cv2.cvtColor(image_in, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "  # Mascara para sorriso\n",
    "  mask_in = np.zeros(image_in.shape[0:2], dtype='uint8')\n",
    "  points = []\n",
    "  if results.multi_face_landmarks:\n",
    "\n",
    "    annotated_image = image_in.copy()\n",
    "    default_image = image_in.copy()\n",
    "\n",
    "    # landmarks do primeiro rosto\n",
    "    face_landmarks = results.multi_face_landmarks[0]\n",
    "    #frozen = frozenset({(270, 409), (317, 402), (81, 82), (91, 181), (37, 0), (84, 17), (269, 270), (321, 375), (318, 324), \n",
    "    # (312, 311), (415, 308), (17, 314), (61, 146), (78, 95), (0, 267), (82, 13), (314, 405), (178, 87), (267, 269), (61, 185), \n",
    "    # (14, 317), (88, 178), (185, 40), (405, 321), (13, 312), (324, 308), (409, 291), (146, 91), (87, 14), (78, 191), (95, 88), (311, 310), (39, 37), (40, 39), (402, 318), (191, 80),\n",
    "    #  (80, 81), (310, 415), (181, 84), (375, 291)})\n",
    "    frozen = frozenset({ (317, 402),(81, 82), (318, 324), (312, 311),(415, 308), (78, 95),(82, 13), (178, 87),\n",
    "     (14, 317), (88, 178), (13, 312), (324, 308),\n",
    "    (87, 14),(78, 191),(95, 88),(311, 310),(402, 318),(191, 80),(80, 81),(310, 415)})\n",
    "    \n",
    "\n",
    "    #for source_idx, target_idx in mp_face_mesh.FACEMESH_LIPS:\n",
    "    color = (255, 0, 0)\n",
    "    radius = 2\n",
    "    # Line thickness of 2 px\n",
    "    thickness = 2\n",
    "    # font\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "      \n",
    "      # org\n",
    "    org = (00, 185)\n",
    "      \n",
    "      # fontScale\n",
    "    fontScale = 1\n",
    "      \n",
    "      # Red color in BGR\n",
    "    color = (0, 0, 255)\n",
    "      \n",
    "      # Line thickness of 2 px\n",
    "    thickness = 2\n",
    "    for source_idx, target_idx in frozen:\n",
    "      \n",
    "\n",
    "\n",
    "      #print(mp_face_mesh.FACEMESH_LIPS)\n",
    "      mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=frozen,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "\n",
    "      #mp_drawing.draw_landmarks(\n",
    "      #      image=default_image,\n",
    "      #      landmark_list=face_landmarks,\n",
    "      #      connections=mp_face_mesh.FACEMESH_LIPS,\n",
    "      #      landmark_drawing_spec=None,\n",
    "      #      connection_drawing_spec=mp_drawing_styles\n",
    "      #      .get_default_face_mesh_contours_style())\n",
    "\n",
    "      source = face_landmarks.landmark[source_idx]\n",
    "      target = face_landmarks.landmark[target_idx]\n",
    "\n",
    "      relative_source = (int(source.x * image_in.shape[1]), int(source.y * image_in.shape[0]))\n",
    "      relative_target = (int(target.x * image_in.shape[1]), int(target.y * image_in.shape[0]))\n",
    "      print(\"source idx {}\".format(source_idx))\n",
    "      print(\"target_idx idx {}\".format(target_idx))\n",
    "      #image_in = cv2.putText(image_in, str(source_idx), relative_source, font, fontScale, \n",
    "      #           color, thickness, cv2.LINE_AA, False)\n",
    "      if source_idx == 87 or source_idx == 82:\n",
    "      #if source_idx == 82:\n",
    "            image_in = cv2.circle(image_in, relative_target, radius, color, thickness)\n",
    "      #image_in = cv2.circle(image_in, relative_source, radius, color, thickness)\n",
    "\n",
    "      points.append(relative_source)\n",
    "      points.append(relative_target)\n",
    "\n",
    "array_np = np.array([[e[0], e[1]] for e in points])  \n",
    "\n",
    "file = open(\"sample.txt\", \"w+\")\n",
    "# Saving the array in a text file\n",
    "content = str(array_np)\n",
    "file.write(content)\n",
    "file.close()\n",
    "\n",
    "cv2.fillConvexPoly(mask_in,cv2.convexHull(array_np),color=(255,255,255))\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "      static_image_mode=True,\n",
    "      max_num_faces=1,\n",
    "      refine_landmarks=True,\n",
    "      min_detection_confidence=0.5) as face_mesh:\n",
    "  # Convert the BGR image to RGB before processing.\n",
    "  results = face_mesh.process(cv2.cvtColor(image_out, cv2.COLOR_BGR2RGB))\n",
    "  face_landmarks = results.multi_face_landmarks[0]\n",
    "  source = face_landmarks.landmark[82]\n",
    "  target = face_landmarks.landmark[87]\n",
    "\n",
    "  relative_source = (int(source.x * image_out.shape[1]), int(source.y * image_out.shape[0]))\n",
    "  relative_target = (int(target.x * image_out.shape[1]), int(target.y * image_out.shape[0]))\n",
    "\n",
    "# The location of the center of the src in the dst\n",
    "width, height, channels = image_out.shape\n",
    "print(image_out.shape)\n",
    "print(relative_source[0])\n",
    "print(relative_target[0])\n",
    "print(int(relative_source[0] +  ((relative_target[0] -  relative_source[0])/2)))\n",
    "y_center = int(relative_source[1] +  ((relative_target[1] -  relative_source[1])/2))\n",
    "#center = (int(relative_source[1] + (relative_source[1] - relative_target[1]) /2), int(width/2))\n",
    "center = (relative_source[0], y_center)\n",
    " \n",
    "# Seamlessly clone src into dst and put the results in output\n",
    "normal_clone = cv2.seamlessClone(image_in, image_out, mask_in, center, cv2.NORMAL_CLONE)\n",
    "mixed_clone = cv2.seamlessClone(image_in, image_out, mask_in, center, cv2.MIXED_CLONE)\n",
    "#mixed_clone = cv2.seamlessClone(obj, im, mask, center, cv2.MIXED_CLONE)\n",
    "\n",
    "\n",
    "#cv2.imshow(\"normal_clone definition\", normal_clone)\n",
    "cv2.imshow(\"default_image definition\", image_in)\n",
    "cv2.imshow(\"annotated_image definition\", normal_clone)\n",
    "\n",
    "  \n",
    "cv2.waitKey(0)\n",
    "# closing all open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# https://www.youtube.com/watch?v=ObmuflX8Ank\n",
    "# https://www.youtube.com/watch?v=Yg6bFRnOSbs\n",
    "# https://www.youtube.com/watch?v=ExToU4KzDm8\n",
    "# https://github.com/numfocus/YouTubeVideoTimestamps\n",
    "# https://github.com/sylwekb/face_swap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (667, 1000, 3)\n",
      "Original img_out Dimensions :  (847, 640, 3)\n",
      "Resized Dimensions :  (847, 640, 3)\n",
      "(847, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    " \n",
    "img = cv2.imread('../data/sorriso.jpeg', cv2.IMREAD_UNCHANGED)\n",
    "img_out = cv2.imread('../data/bush.jpeg', cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "print('Original Dimensions : ',img.shape)\n",
    "print('Original img_out Dimensions : ',img_out.shape)\n",
    " \n",
    "scale_percent = 60 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (img_out.shape[1], img_out.shape[0])\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('Resized Dimensions : ',resized.shape)\n",
    "print(img_out.shape[:2])\n",
    " \n",
    "#cv2.imshow(\"Resized image\", resized)\n",
    "cv2.imshow(\"Out image\", img_out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "112989f40982219f9c2133127490be09f86560db102a13a350eb86b01002b443"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
